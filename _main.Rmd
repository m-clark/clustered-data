--- 
title: <span style="font-size:200%; font-variant:small-caps; font-style:italic; color:#1e90ff">Clustered Data</span>
author:  |
  <div class="title"><span style="font-size:150%; font-variant:small-caps; ">Michael Clark</span><br></div>
  <img src="img/signature-acronym.png" style="width:66%; padding:10px 0;"> <br>
  <img src="img/ARC-acronym-signature.png" style="width:44%; padding:10px 0;"></div>
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
    bookdown::gitbook:
      css: [clustered.css]
      highlight: pygments
      number_sections: false
      config:
        download: ["Clustered_Data.pdf", "Clustered_Data.epub"]
        sharing:
          facebook: false
          twitter: false
        search: yes
        edit: no
        epub: yes
        fontsettings:
          family: sans
          size: 2
      includes:
        before_body: scripts/code_fold.html  # for optional folding of code or output
    # bookdown::tufte_html_book:
    #   toc: yes
    #   css: [toc.css, ../notebooks.css]
always_allow_html: yes
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: m-clark/docs
description: "A comparison of various approaches to dealing with clustered data situations."
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, cache=T, message = F, warning=F, 
                      R.options=list(width=120), fig.align='center')
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(.packages(), 'bookdown', 'knitr', 'rmarkdown'), 'packages.bib')
```

```{r echo=FALSE}
library(tufte); library(tidyverse); library(pander)
```

# 








<!--chapter:end:index.Rmd-->

# Introduction

## What approach to use?

When dealing with <span class="emph">clustered data</span> we have observations that are non-independent and reside within some grouping structure. This could be replicate samples for a specimen in an experiment, children nested within schools, which themselves are nested within districts, we might obtain the same measures at different time points for the same unit of observation, etc.

In such cases standard methods may be applied, just as they can be anywhere, but they would be lacking in some respect.  The following demonstrates a number of methods that might be used in these situations, but while some will definitely be better than others, in other cases, it might be a practical or theoretical choice that drives the decision on how to deal with the data.  In what follows, I will create a data set for us to use so that we don't have any questions as to what might be going on, or what nuances we are not privy to that might produce differences in the results.

## Preliminaries

### Things to note

No attempt is made at providing too much detail on model specifics. It is assumed that if you're reading this you're already at least conceptually familiar in some sense with a couple of these models.  The primary purpose here is to compare, contrast, and note issues.  Sometimes a little more detail is required, but again, the goal is not to teach the methods themselves. See the references for more detail.

In many cases I hide the code but it is still available for viewing, especially in cases where it involves mostly data setup or wrangling, or involves more detailed technique than I am assuming some readers of this doc will understand.  I prefer this document to be a more conceptual in nature and tool agnostic. Though it relies on R, the tool used doesn't matter, and all the models conducted here could be run using other packages.

In describing some of these approaches I suggest that some are more discipline specific or more common in certain areas. This assessment is based on years of consulting across many, many academic disciplines, as well as reading many discipline-specific texts and articles, but this admittedly is not a random sample, so take that info as you wish.  I just provide it as a means for the reader to understand why they may not be familiar with such a technique, and where they might find more information or examples.


#### Terminology

Note that in the following, terms such as 'clustering' and 'cluster' have nothing to do with unsupervised learning methods such as mixture models.  I use the terms 'cluster', 'grouping' and similar terms as synonyms.  As 'mixed' models incorporate random effects, I don't normally distinguish between a *random effects model* (which could be seen as the general case) and a *mixed model*, except perhaps when comparing to 'fixed effects models'.  I'm not going to get into it here, but 'fixed effects' and 'fixed-effect models' are not the same thing[^blameecon].   I will try to be clear.


Color coding:

- <span class="emph">emphasis</span>
- <span class="pack">package</span>
- <span class="func">function</span>
- <span class="objclass">object/class</span>
- [link]()


This doc also available in .epub and .pdf (See the <i class="fa fa-download"></i> icon above), though the doc is automatically converted to those formats, and to be perfectly honest I don't care what may or may not look good or transfer appropriately.

[^blameecon]: Blame the economists.

<!--chapter:end:00_intro.Rmd-->

# Data setup

First we need to set up the data, and to do so we'll consider the longitudinal setting in which individuals are observed over time[^long]. The goal is to create a data set with several (four) observations per individual, i.e. the 'repeated measures' scenario. Aside from variables that represent individual and time point, I will also create an individual level variable, which means that it is constant for each individual. In this case it will be a simple binary variable indicating 'treatment' or 'control' as in an experiment.

The data will also have a certain correlation structure, an autoregressive structure. This means that the residuals of each time point will be notably correlated with the previous, but less so as the time between observations increases. If this is unfamiliar territory to you, just see it as an extra parameter (called rho in the following code) we'll have to estimate and by which you can check the model results against the 'true' value.
 
Here are the main parameters of interest.


```{r setupData1, code_folding='hide'}
library(Matrix)
set.seed(8675309)
tp = 4             # number of timepoints
n = 2500           # number of individuals
sigmasq = 1        # residual variance
rho = .7           # rho of AR1 model

intercept = .2     # intercept
time_beta = .5     # time effect
treat_beta = -.5   # treatment effect

intsd = .5         # intercept standard devation
timesd = .25       # slope of time variable standard deviation
```


Now we can create the data. Here are the main components- time, individual id, and treatment. We will also create an individual specific effect for intercepts and the slope for time.  These are our 'random' effects[^random].

```{r setupData2}
time = rep(0:(tp-1), n)                                          # time
id = rep(1:n, e=tp)                                              # group id
treatment = gl(2, n/2, labels=c('control', 'treatment'))[id]     # cluster level variable
re_int = rnorm(n, sd=intsd)                                      # random intercepts
re_time = rnorm(n, sd=timesd)                                    # random slopes
```


Next we create the residual structure, made somewhat easier since the residual variance is set to 1.  For those interested, this is an autoregressive structure of lag 1. This means that when time points are 1 measure apart, the correlation is $\rho$, at two measures apart, $\rho^2$, at three measures $\rho^3$. You get the gist. This structure is the same within each individual.  Once that is set we draw the residuals based on a multivariate normal with mean zero and covariance based on the structure we have provided.


<div class="fold s">
```{r setupData3}
# create residual 
ar1 = bandSparse(tp, tp, 0:(tp-1), list(rep(1    , tp), 
                                        rep(rho  , tp-1), 
                                        rep(rho^2, tp-2),  
                                        rep(rho^3, tp-3)), symmetric=T) 
Sig = kronecker(diag(1, n), ar1)
Sig[1:10, 1:10]                 # inspect, note that dots are 0s

# e = MASS::mvrnorm(1, mu=rep(0, n*tp), Sigma=sigmasq*Sig)              # residual error
e = c(replicate(n, MASS::mvrnorm(1, mu=rep(0, tp), Sigma=sigmasq*ar1))) # much faster
```
</div>

Now we put it all together and create a data.frame object. A few entries are shown.

```{r setupData4}
# target variable
y = (intercept + re_int[id]) + 
    (time_beta + re_time[id])*time + 
    treat_beta*(treatment=='treatment') + 
    e   

d = data.frame(y, time, treatment, id)
```

```{r showData, echo=F}
library(DT)
datatable(mutate(d, y=round(y, 3)), rownames=F, fillContainer=F, autoHideNavigation=F,  
          options=list(pageLength=10, ordering=F, searching=F, lengthChange=F))
```

<br>

Now we are ready to proceed.


[^long]: This will allow us to examine another technique later, growth curve models, that would not apply to the non-longitudinal case.

[^random]: Note there is nothing *random* about them, they merely represent all causes not identified by the model that vary amongst individuals.  They are an additional source of uncertainty.

<!--chapter:end:01_data_setup.Rmd-->

# Ignore data dependency

The first thing we can do is ignore the situation and just run a standard regression.  This is actually okay if you have very few clusters, and put the cluster id in the model as a fixed effect.  Otherwise, this is not acceptable with regard to the standard errors (SE), as cluster level covariates will be treated as if there are N*timepoint observations (typically underestimating the SE as a result), while the standard error for the time-varying covariates will not account for the clustering (typically overestimating).

```{r SLiM, echo=1}
lm_mod = lm(y ~ time + treatment, data=d, x=T)
pander(summary(lm_mod), round=2, format=list(nsmall=2, digits=2))
```

First be aware that the 'treatmenttreatment' label just tells us that the coefficient refers to moving from the reference group (i.e. 'control') to the treatment group, i.e. considers treatment a binary variable where 1 equals treatment and 0 control.  Note that the coefficients are in the ballpark of where the true values are, save for the estimate of the residual variance, which packs in all sources of variance into one estimate.  As mentioned though, the standard errors for the effects would be problematic.

## Pros

- Easy
- Provides estimation of the effects most are primarily interested in

## Cons

- Standard errors are off
- Ignores the cluster-specific effects, which may be highly interesting

Gist: Probably not viable for most situations.


<!--chapter:end:02_ignore_structure.Rmd-->

# Model every group

We could also run a regression for every group.  This is problematic since we would typically have so little data per group, and too many groups to make sense of.  We'll go ahead and do it anyway.  What if we take the average of all the estimates?

```{r lmList}
library(nlme)
lottalm = lmList(y ~ time | id, d)
colMeans(coef(lottalm))
coef(lm(y ~ time, data=d, x=T))
```

Surprise! We get the same result as if we had simply run the standard linear model (I abbreviate this as SLiM sometimes).  However, we would not get identical coefficients if the data are unbalanced, where individuals may have a different numbers of observations. 

```{r comparelmList, eval=F, echo=FALSE}
idxUB = sample(1:n*tp, 500)
d_UB = d[-idxUB,]
lottalm = lmList(y ~ time | id, d_UB)
colMeans(coef(lottalm), na.rm=T)
# d3heatmap::d3heatmap(coef(lottalm), Rowv=F, Colv=F)
coef(lm(y ~ time, d_UB))
```

```{r meanslope, eval=FALSE, echo=FALSE}
# mean of all possible slopes
n_ = 10000
z = rnorm(n_)
q = .5*z + rnorm(n_, sd=.1)
slopes = rep(NA, n_-1)
for(i in 1:(n_-1)){
  for(j in 1+i){
    slopes[i] = coef(lm(q[c(i,j)] ~ z[c(i,j)] -1))
  }
}
# allpairs = combn(1:n_, 2)
# mean(apply(allpairs, 2, function(x) coef(lm(q[x]~z[x]-1))[1]))
mean(slopes)
coef(lm(q ~ z))[2]
```

In general, this is not the way we want to do things, and one of the biggest drawbacks is that we can never examine cluster level covariates, which may be of key interest, nor is there a straightforward way to do inference for this scenario.  As we will see later, techniques are available that serve as a compromise between these first two alternatives of ignoring the clustering and overfitting to each cluster.

## Pros

- None really. 

## Cons

- Inefficient
- Overly complex
- Overfit at each cluster
- High variance in estimates at each cluster
- Ignores cluster level effects

Gist: While it might be a fun exercise, there is little to be gained by this approach.

<!--chapter:end:03_groupby_model.Rmd-->

# Cluster robust variances

As mentioned, if we ignore the clustering, the so-called fixed effects estimates (i.e. coefficients) are correct, but their variances are not.  We can get proper estimates of the standard errors via <span class="emph">cluster robust standard errors</span>, which are very popular in econometrics and fields trained in that fashion, but not widely used elsewhere in my experience.  Essentially, these allow one to fire-and-forget, and treat the clustering as more of a nuisance.

In programs like Stata, obtaining these are basically an option for most modeling procedures. In R, it's not quite as straightforward, but not difficult. There are packages such as <span class="pack">sandwich</span> that can provide heteroscedastic robust standard errors, but won't necessarily take into account clustering. I provide a custom function that will work in this example so that the curtain can be pulled back a little, but the <span class="pack">plm</span> package would be the way to go for cluster robust standard errors.  The <span class="pack">plm</span> package can take into account the serial autocorrelation via the 'arellano' input to the `type` argument.



```{r plmMod}
library(plm)
# create numeric time so plm won't treat as a factor, time is a reserved name in plm
t2 = d$time                   
clusterrob_mod <- plm(y ~ t2 + treatment, data=d, model='pooling', index='id')
summary(clusterrob_mod)
```

The following compares the different robust SEs we could produce at this point.


<div class="fold s">
```{r clusterRobSe}
resids = resid(lm_mod)                    # residuals
xvar = solve(crossprod(lm_mod$x))         # (X'X)^-1
gd0 = data.frame(id=d$id, r=resids, lm_mod$x) 

# custom function to be applied within cluster
cluscom = function(mat){
  X = as.matrix(mat)[, 3:5]               # magic numbers represent int, time and treatment columns
  crossprod(X, tcrossprod(mat$r)) %*% X
}

# calculate within cluster variance
gd = gd0 %>% 
  group_by(id) %>% 
  do(xvar = cluscom(.))

# plm
plm_se = vcovHC(clusterrob_mod, method='arellano', cluster='group') %>% diag %>% sqrt %>% round(3)

# custom output
custom_se = (xvar %*% Reduce(`+`, gd$xvar) %*% xvar) %>% diag %>% sqrt %>% round(3)

# original lm
lm_se = vcov(lm_mod) %>% diag %>% sqrt  %>% round(3)

# non-clustered 'robust' se
lm_robse = vcovHC(lm_mod, type='HC0') %>% diag %>% sqrt %>% round(3)
```
</div>

```{r seComparison}
# plm
plm_se

# custom output
custom_se

# original lm
lm_se

# non-clustered 'robust' se
lm_robse
```



## Pros

- Don't have to think too much about the issues at play
- Fewer assumptions about the model
- A general heteroscedastic 'robust' approach

## Cons

- Ignores cluster-specific effects
- By default, assumes no intracluster correlation
- Few tools go beyond simple clustering, requiring manual approach
- Not as general as GEE approach
- Suggests that there are problems in model specification that the method itself does not correct

Gist: if your goal is to simply do a standard GLM approach and essentially ignore the clustering, treating it more or less as a nuisance to overcome, and your grouping structure is simple, then this may be for you. However, note that the [GEE][Generalized Estimating Equations] approach as a possibly more flexible alternative that can still incorporate cluster robust standard errors[^clusrob_specialcase].  Furthermore, differences between the robust and regular SE may suggest a model misspecification issue. 'Fixing' standard errors won't solve that problem.


[^clusrob_specialcase]: The cluster robust standard errors as depicted above assume independent error structure in the GEE model, and thus are a special case of GEE.  When you peruse the GEE model, rerun it with the argument `corst='ind'` to duplicate the plm and custom results above.

<!--chapter:end:04_cluster_robust.Rmd-->

# A Survey Approach

I will only briefly mention an approach using survey design to show the similarity of results in that scenario to using cluster robust standard errors. We'll use the <span class="pack">survey</span> package and subsequent <span class="func">svyglm</span> function.

For comparison, we'll use a cluster-based sampling design and nothing more.  This assumes we are sampling clusters from the population of interest for which we want to make inferences to. To use most survey versions of models, the design must be specified a priori.

```{r svy}
library(survey)

design = svydesign(ids=~id, data=d)
svy_mod = svyglm(y ~ time + treatment, data=d, design=design)
summary(svy_mod)
```

These are quite similar to the cluster robust standard errors we got earlier. 

```{r svyclusrob_compare, echo=F}
(xvar %*% Reduce(`+`, gd$xvar) %*% xvar) %>% diag %>% sqrt %>% round(6)
```

In fact, they'd be identical by using a <span class="emph">finite population correction</span> on the latter. 

```{r echo=F, eval=T}
fpc = ((nrow(d) - 1)/(nrow(d)-3))
test = vcovHC(clusterrob_mod, method='arellano', cluster='group') %>% diag %>% sqrt 
test = (xvar %*% Reduce(`+`, gd$xvar) %*% xvar) %>% diag %>% sqrt
round(test*fpc, 6)
```


I only note pros and cons that are relevant for our purposes.  The pros and cons of dealing with survey design in general are quite complex and better hashed out elsewhere.

## Pros

- Can incorporate different and quite complicated sampling designs
- More confidence in inference to the populations of interest

## Cons

- The complexity of incorporating complex design and associated weights
- Beyond simpler settings it can be difficult to tell how best to utilize survey design within the modeling context

Gist: Our goal here was merely to provide a connection to survey design, but that's a whole other situation that will not be considered further.



<!--chapter:end:04b_survey.Rmd-->

# Fixed Effects Models

<span class="emph">Fixed Effects</span> (FE) models are a terribly named approach to dealing with clustered data, but in the simplest case, serve as a contrast to the random effects (RE) approach in which there are only random intercepts[^fe_ext]. Despite the nomenclature, there is mainly one key difference between these models and the 'mixed' models we discuss. Both allow a (random) cluster-specific effect to be added to model, but the FE approach allows that effect to correlate with the predictors, while the RE does not (by default).  In practice however, this means that they may end up being quite different conceptual models as well. As with cluster robust standard errors, economists, and again those trained in that fashion, have historically preferred these models.  In my experience they are rarely used in other disciplines.    

First, let us understand this cluster-specific effect. In the standard regression setting we have a basic intercept, while here, each cluster will provide a nudge above or below that overall effect. Consider the following model (ignoring treatment for now):

$$ y  = \textrm{Int} + \textrm{ClusterEffect} + b*\mathrm{time} + \epsilon  $$

The cluster effect is different from one cluster to the next, but constant for a given cluster.  One way we could perform such a model is just to include `id` as a predictor, thereby getting a unique estimate for each cluster added to the model. In other words, we can also see the situation as if one had simply created a dummy variable for `id` and conducted a standard linear model.  This is in fact how one can think of the FE model, though where the cluster-specific effects are assumed constants to be estimated, and in the past these models were sometimes referred to as least squares dummy variable (LSDV) regression models[^lsdv]. If you actually run the LSDV model, the statistical results for time will be identical.

Why would we be worried about the potential correlation between the cluster-specific effects and the model covariates? In typical social science and economic data it's probably likely that unspecified cluster level effects might have some correlation with the individual level covariates.  This leads to inconsistent estimates in the RE approach, and as such the FE might be used instead.

In the following we use the <span class="pack">plm</span> package to estimate the FE model. I highly recommend reading the excellent [vignette](https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf) for this package if you are one of those econometrically trained folk new to R or the mixed model approach, or conversely, other folk wishing to understand the econometric perspective.

```{r FEmodel}
FE_mod = plm(y ~ as.numeric(time) + treatment, data=d, index='id', model='within')
summary(FE_mod)
```

Note how there is no intercept or treatment effect. In this circumstance of a random intercept model, the FE model can also be seen as a 'demeaning' approach, were the model within a cluster is:

$$y_i-\bar{y_i} = (X_i-\bar{X_i})\beta + (\epsilon-\bar{\epsilon_i})$$


In other words, we subtract the mean from each covariate and response and run the model that way (this is also known as the within transformation). Note the following produces the same result, although the standard error for time is off[^demean].

<div class="fold s">
```{r demean}
d %>% 
  group_by(id) %>% 
  mutate(ybar = y-mean(y),
         timebar = time-mean(time)) %$% 
  lm(ybar ~ timebar) %>% 
  summary
```
</div>

Because of this, if something is constant within a cluster, it drops out of the model, and this includes anything that has only one observation even if the covariate is normally time-varying.  So, not only do you lose the ability to model cluster level effects, though these are 'controlled for', you also lose data. In this we lose the treatment effect entirely, which would be completely unacceptable in most circumstances[^feinteract].



There seem to be philosophical reasons why some prefer FE models that go beyond the practical, because I don't understand the often rigid preference by some adherents over RE models given the drawbacks. I personally have never come across a valid justification for not investigating cluster level covariates if they are available (i.e. almost always in social science, educational, economic, epidemiological and other data, and often would simply include cluster-level averages of available variables). In addition, few of the applications of FE models actually seem interested in the cluster-specific effects, in short treating the clustering as a nuisance, much like the cluster-robust standard error approach[^FEclusterSE].  

## Pros

- Does not assume X and random effects are uncorrelated.

## Cons

- Ignores cluster level covariates or anything cluster constant (i.e. will almost always lose data). 
- Doesn't easily extend to more complex clustering structures.
- Less efficient that RE if RE assumption holds
- Technically one can do random slopes also (mentioned in passing in Greene), but nothing out there does.
- Awkward (my opinion) extension to GLM setting for binary and counts
- More will be pointed out with the mixed models


Gist: If your goal is statistical consistency above all other considerations, this approach is for you. However, given that with mixed models we can potentially overcome the primary issue that the FE model addresses (RE correlated with covariates[^hybrid]), this seems a difficult modeling approach to justify.


[^fe_ext]: Actually, FE models extend beyond this but I've never seen the treatment in textbook presentations, nor am familiar with tools that do so aside from the latent variable approach.

[^FEclusterSE]: Still applies here, i.e. we can still use cluster robust standard errors.

[^lsdv]: This [Stata note](http://www.stata.com/support/faqs/statistics/intercept-in-fixed-effects-model/) highlights the distinction.

[^demean]: This is due to the fact that estimation of the group means was not taken into account.

[^feinteract]: Note that you could still get the interaction of time x treatment, which you'd definitely want to examine in the experimental longitudinal setting. In other circumstances and with numerous covariates, this may become unwieldy, and then there are the issues of when the interaction is not significant, you have no main effect to fall back on, and you're also testing an interaction without all the component main effects.

[^hybrid]: One can use aggregated values of the potentially offending covariates as cluster level covariates. For example, if we had people clustered within political district, we could use average income as a district-level covariate.  Such models are sometimes referred to as hybrids, incorporating both the FE and RE approaches, but this is unwarranted.  All three are simply random effects models of different kinds.

<!--chapter:end:05_fixed_effects_models.Rmd-->

# Mixed Models

<span class="emph">Mixed models</span>, also known by other names, explicitly model the random effects due to the clustering in the data.  They are extremely flexible approaches that can handle crossed and nested clustering structures, as well as different residual dependency structures.  In addition, they can fit within other modeling approaches that can be expressed essentially identically[^remods], allowing for even more modeling considerations.  They also have a very natural extension to Bayesian estimation, and, as we will see later, an alternative 'latent variable' interpretation. In short, there is a lot going on here.

For observations $i$ nested within cluster $c$, we can depict the model in a couple of different ways, but the following is fairly straightforward and in keeping with how the data was created. The main idea is that we have cluster specific coefficients for the intercept and time.

$$ y_{ic} = \beta_{0c} + \beta_{1c} * \mathrm{time}_{ic} + \epsilon_{ic} $$
$$\beta_{0c} = \beta_0 + \beta_2 * \mathrm{Treat_{c}} + \gamma_c$$
$$\beta_{1c} = \beta_1 + \nu_c$$
$$\gamma_c\sim \mathcal{N}(0, \tau^2)$$
$$\nu_c\sim \mathcal{N}(0, \eta^2)$$
$$\epsilon_ic \sim N(0, \sigma^2)$$
Putting the model together we get:

$$ y_{ic} = (\beta_0 + \gamma_c) + (\beta_1 + \nu_c) * \mathrm{time}_{ic} + \beta_2 * \mathrm{Treat_{c}} + \epsilon_{ic} $$
So what we end up with conceptually is the standard linear model, but with cluster specific deviations/effects.  These effects are *random* in that they are drawn from a specific distribution, in this case normal, with mean 0 and some standard deviation, which is estimated as part of the model.  Recall that we specified these values at the [beginning](#preliminaries) though in terms of standard deviation. In addition, the cluster level effect of treatment enters the model as would any other covariate.


To estimate this model we'll use the <span class="pack">nlme</span> package, as it will also allow us to easily estimate the residual correlation structure.  The key part is the `random =` argument, where we note that we want random effects for the intercept and time.

```{r nlme, echo=1:2}
library(nlme)
mixed_mod = lme(y ~ time + treatment, random = ~1+time|id, correlation=corAR1(form=~time|id))
pander(summary(mixed_mod), round=3, nsmall=3)

# This is ridiculous
VarCorr(mixed_mod, rdig=3)[,1:2] %>%             # note: you have removed the correlation from display
  as_data_frame() %>% 
  mutate(Variance=as.numeric(Variance), StdDev=as.numeric(StdDev)) %>% 
  mutate_all(round, digits=3) %>% 
  pander(round=3, nsmall=3)

coef(mixed_mod$modelStruct$corStruct, unconstrained=F) %>% 
  pander(round=3, nsmall=3)  
```

The results show that the primary regression coefficients, known in the mixed model literature as 'fixed effects', are reasonably recovered, as are the standard deviations for the random effects (recall `intsd`, `timesd`, and `sigmasq`). In addition, `Phi` estimates what we were calling `rho`.  Furthermore, there is a low estimated correlation between intercepts and slopes (not shown), in keeping with how the data was generated.

A common method for determining whether to use the FE vs. RE is the <span class="emph">Hausman test</span>, but it comes with its own assumptions and the standard version is problematic except in the simplest of settings. The plm package provides this if you want to try it, but will only be applicable to the random intercepts scenario. It tests whether the assumption underlying the random effects approach is viable, and if rejected, one would go with the FE model. As it is an 'accept the null' test, it is fundamentally illogical, to go along with the other issues.

```{r hausman, echo=FALSE, eval=FALSE}
feMod = plm(y ~ as.numeric(time) + treatment, data=d, index='id', model='within')
reMod = plm(y ~ as.numeric(time) + treatment, data=d, index='id', model='random')
phtest(feMod, reMod, method='aux', vcov=vcovHC)
```


It may be instructive to compare the random coefficients to the approach where we conducted a model for each individual.

```{r compareMixedByGroup, echo=FALSE}
library(ggplot2)
coefllm = coef(lottalm); colnames(coefllm) = c('Int', 'Time')
coefRE = coefficients(mixed_mod)[,-3]; colnames(coefRE) = c('Int', 'Time')  # no cluster covariate
g = rbind(coefllm, coefRE) %>% 
  mutate(Model = rep(c('By Group', 'Mixed Model'), e=n)) %>% 
  gather(key=Context, value=Coefficient, -Model) %>% 
  ggplot(aes(x=Coefficient, fill=Model, color=Model)) +
  geom_density(alpha=.5) +
  facet_wrap(~Context) +
  lazerhawk::theme_trueMinimal() 
library(plotly)
ggplotly(tooltip='none', width='auto') %>% layout()
```

<br>

What the visualization makes clear is that the mixed model has a regularizing effect on the coefficients relative to the by-group model, shrinking the coefficients toward the population average. The by-group approach overfits, treating each individual as a unique snowflake, while the mixed model recognizes that they might have things in common. Practically and philosophically this is quite appealing.

## Pros

- Examine cluster-specific effects
- Can retain cluster level variable effects
- More efficient than FE (and cluster robust approach) if assumptions hold
- Easily incorporate additional sources of variance
- Additional sources of variance may be due to nested or crossed grouping structure and even interactions of random effects
- Can model residual dependency
- Can allow any coefficient to be 'random'
- Close ties to other modeling approaches

## Cons

- Inconsistent estimates if predictors are correlated with random effect(s) and other steps not taken
- May have difficult estimation in the generalized case, e.g. logistic model (my experience)

Gist: Basically the mixed model can provide everything you need, and not necessarily at the cost of consistency.



[^remods]: Such as spatial random effects, additive models, network effects etc. See the Fahrmeier et al. reference.

<!--chapter:end:06_mixed_models.Rmd-->

# Generalized Estimating Equations

A <span class="emph">generalized estimating equation</span> is an *estimation procedure*[^notamodel] for dealing with clustered data, and is seemingly very popular in disciplines trained with a biostatistics perspective, but perhaps not too commonly used elsewhere.  Models using this approach are sometimes called <span class="emph">marginal models</span>,  and can be seen as follows, where the target $y$ is multivariate normal with mean vector $X\beta$ and covariance matrix $\Sigma$, which is typically block diagonal as we created at the beginning[^geevsre].

$$ y \sim \mathcal{N}(X\beta,  \Sigma) $$


Here the focus is on the 'population average' effects, akin to 'fixed' effects in the RE model, and in some circumstances they are identical.  In addition, one specifies the type of covariance structure thought to underlie the data. Among the more common are:

- *independence*: no correlation
- *autoregressive*: as described previously
- *exchangeable*: same correlation everywhere (aka 'compound symmetry' or 'spherical')
- *unstructured*: all possible correlations are estimated as parameters

And there are many others. The GEE approach is identical to RE intercept-only model approach if one conducts a linear Gaussian model, as in this case. In addition, these correlation structures are often available to mixed models tools, so this extension alone should not be a reason[^lme4] to use the GEE approach.

We'll use the <span class="pack">geepack</span> package in order to conduct the gee approach to the model.  We'll specify an autoregressive correlation structure as we did with the mixed model, and as the data was in fact designed with.

```{r geeMod, echo=-(4:6)}
library(geepack)

gee_mod = geeglm(y ~ time + treatment, data=d, corstr='ar1', id=id, waves=time)
summary(gee_mod)$coefficients %>% pander(digits=3, round=3)
data_frame(`Residual Variance` = summary(gee_mod)$dispersion$Estimate) %>% round(3) %>%  pander
summary(gee_mod)$corr %>% pander(round=3)
```

We should be getting used to the coefficient estimates for the population-average, a.k.a. fixed effects, by now.  We also obtain an estimate for the residual correlation, here noted as *alpha*.

Note the similarities here compared with the mixed model where there is only a random intercept. A couple of changes are made to keep things as similar as possible.

```{r mixedgeeComparison, echo=1}
mixed_mod_ri_only = lme(y ~ time + treatment, data=d, random = ~1|id, cor=corAR1(form=~time), 
                        control=lmeControl(opt='optim'), method='ML')  # changed opt bc nlminb had issues
summary(mixed_mod_ri_only) %>% pander(digits=3, round=3)
VarCorr(mixed_mod_ri_only, rdig=3)[,1:2] %>%             # note: you have removed the correlation from display
  as_data_frame() %>% 
  mutate(Variance=as.numeric(Variance), StdDev=as.numeric(StdDev)) %>% 
  mutate_all(round, digits=3) %>% 
  pander(round=3, nsmall=3)

coef(mixed_mod_ri_only$modelStruct$corStruct, unconstrained=F) %>% 
  pander(round=3, nsmall=3)  
```

The population average effects are identical (though the <span class="func">geeglm</span> function automatically does cluster robust standard errors). The estimated correlations for both are similar, and a bit high.  There is essentially no cluster variance in the mixed model, and both estimated residual variances are similar, and similar to the standard linear model we started with.  This makes sense as the variance is equal to the residual variance + the intercept variance + slope variance.


```{r geelmelmvar}
summary(lm_mod)$sigma^2                   # similar to gee and random intercept only
sum(as.numeric(VarCorr(mixed_mod)[,1]))   # model that incorporates all sources of variance
intsd^2 + timesd^2 + sigmasq              # 'truth'
```


GEE models are generally robust to misspecification of the covariance structure.  They are rarely implemented for more than simple clustering but some tools allow for it[^kerby].


## Pros

- Easy modeling of different correlation structures
- Focus on population level effects
- Extends the cluster robust approach to glm setting and other correlation structures[^clusrob_recall]
- Robust to misspecification of correlation structure
- May be more feasible with larger data situations than mixed models

## Cons

- Cluster-specific effects are not estimated
- Not easily extendable to other clustering situations, e.g. nested
- Missing data is assumed Missing Completely at Random (MCAR) (RE and FE assume MAR)

Gist: If your goal is to focus on population average effects and ignore subject specific effects, without the drawbacks of the FE model, this might be considered



[^notamodel]: Not a model!  We're just doing a GLM here.

[^lme4]: <span class="pack">lme4</span> is a very widely used mixed model package that does not allow the specification of a correlation structure for the residuals.

[^geevsre]: The corresponding matrix formulation for the *conditional* model of the random effects approach is:
$$ y \sim \mathcal{N}(X\beta + Z\Gamma,  \Sigma) $$
where $Z$ is some subset of $X$, and $\Gamma$ contains the random effects.

[^kerby]: CSCAR director [Kerby Shedden](https://github.com/kshedden) has worked on a Python implementation for nested structures as part of the [statsmodels module](http://statsmodels.sourceforge.net/devel/gee.html).

[^clusrob_recall]: Recall the note at the end of the [cluster robust SE chapter][Cluster robust variances].

<!--chapter:end:07_gee.Rmd-->

# Latent Growth Curve

An alternative approach to mixed models considers the random effects as <span class="emph">latent variables</span> with the outcome at each time point an indicator for the latent variable.  I have details [elsewhere](https://m-clark.github.io/docs/sem), but I want to explore this as it is a commonly used technique in the social sciences, especially psychology.  <span class="emph">Latent Growth Curve Models</span> are a special case of <span class="emph">structural equation modeling</span>, a highly flexible tool that can incorporate latent variables, indirect effects, multiple outcomes etc.  Growth curve models are actually somewhat irregular SEM in the way that they are specified, but for our purposes, we only want to see how the approach works and compare it to previous methods.  

The first thing is that the data has to be in *wide* format, such that we have one column per time point, and thus only one row per individual.  Once the data is ready we specify the model syntax.  By default, the SEM approach also assumes unequal variances across time, so to make it more comparable, we fix that value to be constant.  We'll use <span class="pack">lavaan</span> to estimate the model.

<div class="fold s">
```{r dataWide}
dwide = spread(d, key=time, value=y, sep='_') %>% 
  mutate(treatment = treatment=='treatment')  # otherwise converted to numeric directly as 1-2 instead of 1-0
head(dwide)
```
</div>


<div class="fold s">
```{r lavaanSyntax}
growthmod_syntax = "
# model for the intercept and slope latent variables
  int   =~ 1*time_0 + 1*time_1 + 1*time_2 + 1*time_3
  slope =~ 0*time_0 + 1*time_1 + 2*time_2 + 3*time_3

# cluster-level effect
  int ~ treatment

# intercept-slope correlation
  int ~~ slope

# fix to equal variances (parameter 'res')
  time_0 ~~ res*time_0
  time_1 ~~ res*time_1
  time_2 ~~ res*time_2
  time_3 ~~ res*time_3
"
```
</div>


```{r lavMod}
library(lavaan)
growth_mod = growth(growthmod_syntax, data=dwide)
summary(growth_mod, standardized=T)
```

The `Intercepts:` section of the output shows what would be the fixed effects in the mixed model, and in this case, they are in fact 'intercepts' in this latent variable approach, so that is why they are named as such.  The Regression of `int` on treatment depicts the treatment effect, and will make more sense to those who come to mixed models from the <span class="emph">multilevel modeling</span> literature. If you go back to the model depiction for the mixed model, this model more explicitly denotes $\beta_{0c} = \beta_0 + \beta_2*\textrm{Treatment} + \gamma_c$.  The `res` parameter is the arbitray name I've given for the residual variance, and is roughly equivalent to the square of the residual standard deviation in the mixed model output.  The above model does not allow for correlated residuals, though this is possible[^semnotes].  

The primary point here is not to precisely reproduce the correct model but to show the identity between the mixed model and the latent growth curve approach.  Proper specification will lead to identical results between latent growth curve and mixed models. The following creates a mixed model that is the equivalent.


```{r growthMixedCompare, echo=1}
mixed_mod_nocorr = lme(y ~ time + treatment, data=d, random=~1+time|id, method="ML")
summary(mixed_mod_nocorr) %>% pander(round=3, nsmall=3)
VarCorr(mixed_mod_nocorr, rdig=3)[,1:2] %>%             # note: you have removed the correlation from display
  as_data_frame() %>% 
  mutate(Variance=as.numeric(Variance), StdDev=as.numeric(StdDev)) %>% 
  mutate_all(round, digits=3) %>% 
  pander(round=3, nsmall=3)
```


## Pros

- Can be utilized on less data than typical SEM
- Very efficient estimation
- Can deal with very complex models, including mediation, parallel processes etc.

## Cons

- Tedious to specify even the simplest of models
- Very tedious to specify even common extensions (e.g. time-varying covariates)
- Even worse to get into correlated residuals
- More complex cluster structure is not dealt with well (if at all)[^mplus]
- Assumes balanced time points
- Doesn't deal with many time points well (if also time-varying covariates especially)

Gist: Growth curve models are very flexible, but they are also problematic simply because they are from the SEM world, which is one where models are notoriously misapplied.  Furthermore, there are no common uses of growth curve models that would not be more easily implemented in one of several R packages[^growthtomixed] and various other languages and statistical programs.  While I find the latent variable interpretation very much intriguing,  the latent variable approach is not something I'd normally consider for this setting.


[^semnotes]: See my LGC chapter in this [SEM document](https://m-clark.github.io/docs/sem).  Once you see it there you'll know why I did not do so here.

[^growthtomixed]: See the <span class="pack">mediation</span> package for mediation with mixed models, <span class="pack">flexMix</span> for growth mixture models, Bayesian approaches for parallel processes etc.

[^mplus]: MPlus has [recently](https://www.statmodel.com/download/handouts/MuthenV7Part3.pdf) incorporated the ability to handle crossed random effects (and see example 9.24 in the version 7 manual), but I have no idea how they work in realistic situations with potentially many, possibly time-varying, covariates, and it's actually done with their multilevel approach rather than the LGC we've been discussing.  Furthermore, it requires the Bayesian estimator, which, if you're going that route you might as well use <span class="pack">rstan</span>, <span class="pack">rjags</span> or similar and have a lot more utility (and clarity) at your disposal.  For tools like <span class="pack">lme4</span> and similar, incorporating crossed random effects are no more difficult than other situations, i.e. are 1 line of code, while you'd be debugging the MPlus output for days.

<!--chapter:end:08_latent_growth_curves.Rmd-->

# Summary

Let's revisit the results all at once. Seeing the output side-by-side may prove helpful in comparing the different approaches.

## Fixed Effects

We'll start with the 'fixed effects' or 'population average' estimates. It may be worth sorting by `Effect`[^lmgroupedSE].  I also include an random intercept only model for a more direct comparison between the RE and FE models.

```{r feEstimates, echo=F}
lmResults = as_data_frame(summary(lm_mod)$coefficients[,1:2]); colnames(lmResults)[2]='SE'
lmGroupedResults = data_frame(Estimate=colMeans(coef(lottalm)), SE=apply(coef(lottalm), 2, sd)/sqrt(n))
clusterrobResults = data_frame(Estimate=summary(clusterrob_mod)$coefficients[,1], 
                               SE=vcovHC(clusterrob_mod, method='arellano', cluster='group') %>% diag %>% sqrt)
FEResults = as_data_frame(summary(FE_mod)$coefficients[,1:2, drop=F]); colnames(FEResults)[2]='SE'
REResults_intOnly = as_data_frame(summary(mixed_mod_ri_only)$tTable[,1:2]); colnames(REResults_intOnly)= c('Estimate', 'SE')
REResults = as_data_frame(summary(mixed_mod)$tTable[,1:2]); colnames(REResults)= c('Estimate', 'SE')
GEEResults = as_data_frame(summary(gee_mod)$coefficients[,1:2]); colnames(GEEResults)[2]='SE'
GCResults = data_frame(Estimate=growth_mod@ParTable$est[c(23:24, 9)], SE=growth_mod@ParTable$se[c(23:24, 9)])
# see parTable(growth_mod)

coef_results = bind_rows(list(lmResults, lmGroupedResults, clusterrobResults,
                               FEResults, REResults_intOnly, REResults, 
                              GEEResults, GCResults)) %>% 
  mutate(SE = format(SE, digits=1, nsmall=3),
         Estimate = format(Estimate, digits=1, nsmall=3)) %>% 
  mutate(Model = c(rep('LM',3), 'LM_Grouped', 'LM_Grouped', rep('Cluster Robust', 3),
                   'FE', rep('RE_int_only', 3), rep('RE', 3), rep('GEE', 3), rep('Growth',3)),
         Effect = c('Intercept', 'time', 'treatment', 'Intercept','time', 'Intercept', 'time', 'treatment',
                    'time','Intercept', 'time', 'treatment','Intercept', 'time', 'treatment', 
                    'Intercept', 'time', 'treatment', 'Intercept', 'time', 'treatment')) %>% 
  select(Model, Effect, Estimate, SE) # rearrange columns

datatable(coef_results, rownames=F, fillContainer=F, autoHideNavigation=F,  
          options=list(pageLength=18, ordering=T, searching=F, lengthChange=F, caption=F, paging=F))
```

<br>

The main thing to note is that we would come to no grand differences in substantive conclusions, except for the Grouped and FE approach, where we can come to no conclusion about the treatment effect.  Even statistically, the conclusions would be the same, so what can one conclude about these main effects?

For one, we typically don't have 10000 observations for our models, and so the differences would be more notable in that case.  I invite you to rerun the simulation with a smaller sample size where n = 100 individuals instead of 2500, as well as with smaller effects (these are large), and see what you come up with.  In general, with a lot of data you shouldn't come to wildly different conclusions with different techniques[^moardata].

While I have yet to come across a client that actually cares what the precise value of the standard error is, many care about statistical significance.  If that is of primary concern, and it shouldn't be, then one would prefer techniques that get better estimates of the standard error[^seproblems].  In that sense, a cluster robust approach would help, but would be better if applied in the FE/RE/GEE model settings.


## Variance estimates

Here we'll compare variance estimates.  I add mixed and gee models with no residual correlation estimate to make more direct comparisons to the growth curve model, as well as an random intercept only with no dependency structure assumed (other than independence) for comparison to the FE model.

```{r varEstimates, echo=FALSE}
gee_mod_nocorr = geeglm(y ~ time + treatment, data=d, corstr='independence', id=id, waves=time)
mixed_mod_ri_nocorr = lme(y ~ time + treatment, random = ~1|id)

lmResults = data_frame(`Residual Var.` = summary(lm_mod)$sigma^2, `Intercept Var.`=NA, `Time Var`=NA, `Residual Cor`=NA)

lmGroupedResults = data_frame(`Residual Var.` = NA, `Intercept Var.`=NA, `Time Var`=NA, `Residual Cor`=NA)

clusterrobResults = data_frame(`Residual Var.` = c(crossprod(clusterrob_mod$residuals)/clusterrob_mod$df.residual),
                               `Intercept Var.`=NA, `Time Var`=NA, `Residual Cor`=NA)

FEResults = data_frame(`Residual Var.` = c(crossprod(summary(FE_mod)$residuals)/summary(FE_mod)$df.residual), 
                       `Intercept Var.`=NA, `Time Var`=NA, `Residual Cor`=NA)

REResults_intOnly = data_frame(`Residual Var.` =VarCorr(mixed_mod_ri_only)[2,1], 
                               `Intercept Var.`=VarCorr(mixed_mod_ri_only)[1,1], 
                               `Time Var`= NA, 
                               `Residual Cor`=coef(mixed_mod_ri_only$modelStruct$corStruct, unconstrained=F)) %>% 
  mutate_all(as.numeric)
REResults_intOnly_noCorr = data_frame(`Residual Var.` =VarCorr(mixed_mod_ri_nocorr)[2,1], 
                               `Intercept Var.`=VarCorr(mixed_mod_ri_nocorr)[1,1], 
                               `Time Var`= NA, 
                               `Residual Cor`=NA) %>% 
  mutate_all(as.numeric)
REResults_noCorr = data_frame(`Residual Var.` =VarCorr(mixed_mod_nocorr)[3,1], 
                              `Intercept Var.`=VarCorr(mixed_mod_nocorr)[1,1], 
                              `Time Var`= VarCorr(mixed_mod_nocorr)[2,1], 
                              `Residual Cor`=NA) %>% 
  mutate_all(as.numeric)
REResults = data_frame(`Residual Var.` =VarCorr(mixed_mod)[3,1], 
                       `Intercept Var.`=VarCorr(mixed_mod)[1,1], 
                       `Time Var`=VarCorr(mixed_mod)[2,1], 
                       `Residual Cor`=coef(mixed_mod$modelStruct$corStruct, unconstrained=F)) %>% 
  mutate_all(as.numeric)

GEEResults_nocorr = data_frame(`Residual Var.` = summary(gee_mod_nocorr)$dispersion$Estimate, `Intercept Var.`=NA, `Time Var`=NA, 
                               `Residual Cor`= NA)
GEEResults = data_frame(`Residual Var.` = summary(gee_mod)$dispersion$Estimate, `Intercept Var.`=NA, `Time Var`=NA, 
                        `Residual Cor`= summary(gee_mod)$corr$Estimate)

GCResults = data_frame(`Residual Var.` = growth_mod@ParTable$est[11], `Intercept Var.`=growth_mod@ParTable$est[15], 
                       `Time Var`=growth_mod@ParTable$est[16], `Residual Cor`=NA)

var_results = bind_rows(list(lmResults, lmGroupedResults, clusterrobResults, FEResults, 
                             REResults_intOnly_noCorr, REResults_intOnly, REResults_noCorr, REResults, 
                             GEEResults_nocorr, GEEResults, GCResults)) %>% 
  mutate_all(format, digits=1, nsmall=3) %>% 
  mutate(Model = c('LM', 'LM Grouped','Cluster Robust','FE', 
                   'RE_int_only_nocorr', 'RE_int_only_corr','RE_nocorr', 'RE_full', 
                   'GEE_nocorr', 'GEE', 'Growth')) %>% 
  select(Model, contains('v'), contains('cor')) # rearrange


datatable(var_results, rownames=F, fillContainer=F, autoHideNavigation=F,  
          options=list(pageLength=18, ordering=T, searching=F, lengthChange=F, caption=F, paging=F))
```

<br>

Cluster robust modifications alone will change nothing compared to a SLiM except for the standard errors of the main effects. A GEE approach with an independence structure and constant residual variance is equivalent to the SLiM. The FE residual variance is equivalent to the SLiM if `id` had been included in the model, and is equivalent to that estimated by an RE intercept only model with no residual dependency estimated.  The estimates of RE model with random intercepts and slopes but no correlation structure assumed are identical to the growth curve estimates.

## Conclusion

This is only a single and contrived data example that is based on random effects as part of the underlying data generating process.  If there is no clustering effect, then the standard linear model would obviously the way to go, but that is also not the situation we're interested in.  If there is no residual correlation, or only cluster-specific effects akin to an 'intercept-only' model, little changes from what has been noted above, aside from latent growth curve models being less of an option, and they wouldn't be in a non-longitudinal setting anyway.  In general, the same issues noted above would still be in play for the most part for simpler settings.

Specifically, I think it's safe to say that the standard linear model approach has drawbacks and is simply not necessary given how easy it is to conduct more appropriate methods.  Using cluster robust standard errors may help, but ignores what could potentially be a very rich investigation of cluster specific effects, and may be best used as a diagnostic or signal for a misspecified model.  I also think it's difficult to justify the FE approach where we can't even investigate cluster level covariates, but again, that is my bias, and may not be shared by others.  Latent growth curve models will probably only be useful if one is dealing with very complex SEM, and which in that case, one will have a host of other issues to contend with that aren't applicable in this setting.  Otherwise, LGCM will be identical to mixed models, and even some more complex than depicted, and may only make for more tedious coding with little else gained.

That leaves the mixed model and GEE GLM approaches. Between these two, we don't even have to choose if it's a random intercept, linear model, as they would have similar 'population-average' interpretations, and similar estimates of like parameters, though the mixed model provides cluster-specific effects and predictions.  Interpretations do change with nonlinear models such as with a binary outcome and logit link, but see the Gardiner reference below for how the estimated coefficients relate to one another in those cases.

Only the mixed model provides cluster-specific effects and predictions, while allowing for complex cluster structures and retaining cluster level covariates. What's more, they can be specified to overcome the primary motivation for preferring FE models. In addition, they readily extend to other types of 'random effects' models (e.g. spatial).

In conclusion then, the random effects/mixed model approach can provide most everything one could want in dealing with a variety of clustered data situations. It is highly flexible, and a very powerful tool to have at one's disposal.


[^lmgroupedSE]: For the grouped SE, I just calculated as one would the mean `sd/sqrt(n)`.  I thought about using Rubin's rules as one would for the missing value situation, but each four observation linear model has very high variance. This approach puts it in the ball park of the other estimates.

[^seproblems]: Note that when there are multiple sources of variance, it is difficult to know what the standard error should be. Programs like Stata and SAS make the decision for you, and provide approximate p-values (via Kenward-Roger or other approximation), while the R package <span class="pack">lme4</span> will not provide p-values.  One can get decent interval estimates (e.g. via bootstrap or MCMC), but see the R wiki reference for some details on this issue.

[^moardata]: "More data beats a cleverer algorithm." ~  Pedro Domingos

<!--chapter:end:1000_summary.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`



These references are a mix of starting points, interesting notes, and more authoritative sources.

## Robust SE

- King & Roberts. (2015). How Robust Standard Errors Expose Methodological Problems They Do Not Fix. *Political Analysis*.
- Trivedi, C. (2011). [Robust Inference with Clustered Data](http://www.stata.com/meeting/mexico11/materials/cameron.pdf). 


## Fixed effects and 'panel' data models

- Wooldridge. (2016). Introductory Econometrics: A Modern Approach (6e). [link](http://www.cengage.com/search/productOverview.do?N=16+4294922239+4294966644+142&Ntk=P_EPI&Ntt=152961460856007931617237609421833777028&Ntx=mode%2Bmatchallpartial)
- Wooldridge. (2010). Econometric Analysis of Cross Section and Panel Data (2e). [link](https://mitpress.mit.edu/books/econometric-analysis-cross-section-and-panel-data)
- Baltalgi. (2005). Econometric Analysis of Panel Data (3e). [link](http://www.wiley.com/legacy/wileychi/baltagi3e/)


## Mixed models

- [My overview](https://m-clark.github.io/docs/mixedModels/mixedModels.html).
- Gelman & Hill. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. [link](http://www.stat.columbia.edu/~gelman/arm/)
- Pinheiro & Bates. (2000). Mixed-Effects Models in S and S-PLUS. [link](http://link.springer.com/book/10.1007%2Fb98882)
- Fahrmeier et al. (2013). Regression. [link](http://www.springer.com/us/book/9783642343322)
- West et al. (2014). Linear Mixed Models: A Practical Guide Using Statistical Software (2e). [link](http://www-personal.umich.edu/~bwest/almmussp.html) 


## GEE

To be honest I can't speak to this reference from experience, though I've read and would recommend Hardin and Hilbe's GLM book, and this is a similar approach (applied with Stata examples).

- Hardin & Hilbe (2013). Generalized Estimating Equations. [link](https://www.crcpress.com/Generalized-Estimating-Equations-Second-Edition/Hardin-Hilbe/p/book/9781439881132) 


## Growth Curve Models

- Kline. (2015). Principles and Practice of Structural Equation Modeling. [link](http://www.guilford.com/books/Principles-and-Practice-of-Structural-Equation-Modeling/Rex-Kline/9781462523344)

- [My SEM notes](http://m-clark.github.io/docs/sem/latent-growth-curves.html)


## Comparison/Issues

- Gardiner et al. (2009). Fixed effects, random effects, and GEE: What are the differences? *Statistics in Medicine*. [link](https://www.ncbi.nlm.nih.gov/pubmed/19012297)

- Bell & Jones. (2015) Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data.  [link](https://www.cambridge.org/core/journals/political-science-research-and-methods/article/explaining-fixed-effects-random-effects-modeling-of-time-series-cross-sectional-and-panel-data/0334A27557D15848549120FE8ECD8D63)

- Bell & Jones. (2016) Fixed and Random effects: making an informed choice.[link](http://seis.bris.ac.uk/~ggmhf/ABMFKJ.FE-REdraft.pdf)

- R mixed list FAQ. Old but still has useful information. [link](http://glmm.wikidot.com/faq)

<!--chapter:end:1001_references.Rmd-->

